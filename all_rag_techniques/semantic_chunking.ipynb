{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co33LgbQJXjE"
      },
      "source": [
        "# Semantic Chunking for Document Processing\n",
        "\n",
        "## Overview\n",
        "\n",
        "This code implements a semantic chunking approach for processing and retrieving information from PDF documents, [first proposed by Greg Kamradt](https://youtu.be/8OJC21T2SL4?t=1933) and subsequently [implemented in LangChain](https://python.langchain.com/docs/how_to/semantic-chunker/). Unlike traditional methods that split text based on fixed character or word counts, semantic chunking aims to create more meaningful and context-aware text segments.\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Traditional text splitting methods often break documents at arbitrary points, potentially disrupting the flow of information and context. Semantic chunking addresses this issue by attempting to split text at more natural breakpoints, preserving semantic coherence within each chunk.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. PDF processing and text extraction\n",
        "2. Semantic chunking using LangChain's SemanticChunker\n",
        "3. Vector store creation using FAISS and OpenAI embeddings\n",
        "4. Retriever setup for querying the processed documents\n",
        "\n",
        "## Method Details\n",
        "\n",
        "### Document Preprocessing\n",
        "\n",
        "1. The PDF is read and converted to a string using a custom `read_pdf_to_string` function.\n",
        "\n",
        "### Semantic Chunking\n",
        "\n",
        "1. Utilizes LangChain's `SemanticChunker` with OpenAI embeddings.\n",
        "2. Three breakpoint types are available:\n",
        "   - 'percentile': Splits at differences greater than the X percentile.\n",
        "   - 'standard_deviation': Splits at differences greater than X standard deviations.\n",
        "   - 'interquartile': Uses the interquartile distance to determine split points.\n",
        "3. In this implementation, the 'percentile' method is used with a threshold of 90.\n",
        "\n",
        "### Vector Store Creation\n",
        "\n",
        "1. OpenAI embeddings are used to create vector representations of the semantic chunks.\n",
        "2. A FAISS vector store is created from these embeddings for efficient similarity search.\n",
        "\n",
        "### Retriever Setup\n",
        "\n",
        "1. A retriever is configured to fetch the top 2 most relevant chunks for a given query.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. Context-Aware Splitting: Attempts to maintain semantic coherence within chunks.\n",
        "2. Flexible Configuration: Allows for different breakpoint types and thresholds.\n",
        "3. Integration with Advanced NLP Tools: Uses OpenAI embeddings for both chunking and retrieval.\n",
        "\n",
        "## Benefits of this Approach\n",
        "\n",
        "1. Improved Coherence: Chunks are more likely to contain complete thoughts or ideas.\n",
        "2. Better Retrieval Relevance: By preserving context, retrieval accuracy may be enhanced.\n",
        "3. Adaptability: The chunking method can be adjusted based on the nature of the documents and retrieval needs.\n",
        "4. Potential for Better Understanding: LLMs or downstream tasks may perform better with more coherent text segments.\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "1. Uses OpenAI's embeddings for both the semantic chunking process and the final vector representations.\n",
        "2. Employs FAISS for creating an efficient searchable index of the chunks.\n",
        "3. The retriever is set up to return the top 2 most relevant chunks, which can be adjusted as needed.\n",
        "\n",
        "## Example Usage\n",
        "\n",
        "The code includes a test query: \"What is the main cause of climate change?\". This demonstrates how the semantic chunking and retrieval system can be used to find relevant information from the processed document.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Semantic chunking represents an advanced approach to document processing for retrieval systems. By attempting to maintain semantic coherence within text segments, it has the potential to improve the quality of retrieved information and enhance the performance of downstream NLP tasks. This technique is particularly valuable for processing long, complex documents where maintaining context is crucial, such as scientific papers, legal documents, or comprehensive reports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYQuGs3KJXjG"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"https://github.com/NirDiamant/RAG_Techniques/blob/main/images/semantic_chunking_comparison.svg?raw=1\" alt=\"Self RAG\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THNjCYx-JXjH"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "finGw_ZVJXjH",
        "outputId": "4830fffe-29f5-44b4-80ff-6d9280bad3bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.7)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.7)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: fitz in /usr/local/lib/python3.12/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.15.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.13.0)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.12/dist-packages (from fitz) (5.0.9)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.12/dist-packages (from fitz) (7.2.0)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.12/dist-packages (from fitz) (0.31.1)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (from fitz) (5.3.3)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.12/dist-packages (from fitz) (1.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from fitz) (2.2.2)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.12/dist-packages (from fitz) (1.6.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from fitz) (1.16.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2->fitz) (3.3.1)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (8.3.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.6.1)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (2.1.1)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (4.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (2.9.0.post0)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (7.5.0)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.20.2)\n",
            "Requirement already satisfied: traits>=6.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (7.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (3.20.3)\n",
            "Requirement already satisfied: acres in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (0.5.0)\n",
            "Requirement already satisfied: etelemetry>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (0.3.1)\n",
            "Requirement already satisfied: looseversion!=1.2 in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (1.3.0)\n",
            "Requirement already satisfied: puremagic in /usr/local/lib/python3.12/dist-packages (from nipype->fitz) (1.30)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->fitz) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->fitz) (2025.3)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.12/dist-packages (from pyxnat->fitz) (6.0.2)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.12/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.12/dist-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install langchain-experimental langchain-openai python-dotenv langchain-community langchain-core rank-bm25 fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2swUAv7SJXjI",
        "outputId": "e6e2ff1a-c1b3-4aaa-b341-4b01273077eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RAG_TECHNIQUES'...\n",
            "remote: Enumerating objects: 1765, done.\u001b[K\n",
            "remote: Counting objects: 100% (1101/1101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (414/414), done.\u001b[K\n",
            "remote: Total 1765 (delta 733), reused 688 (delta 687), pack-reused 664 (from 4)\u001b[K\n",
            "Receiving objects: 100% (1765/1765), 36.51 MiB | 34.52 MiB/s, done.\n",
            "Resolving deltas: 100% (1119/1119), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository to access helper functions and evaluation modules\n",
        "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "collapsed": true,
        "id": "NByAf7CtJXjJ",
        "outputId": "7dd600dd-33c8-4f83-9d77-9d1b49de7bc6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain.document_loaders'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-282106005.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Original path append replaced for Colab compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalute_rag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/RAG_TECHNIQUES/helper_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.document_loaders'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "from helper_functions import *\n",
        "from evaluation.evalute_rag import *\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# # Load environment variables from a .env file\n",
        "# load_dotenv()\n",
        "\n",
        "# # Set the OpenAI API key environment variable\n",
        "# os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "openai_api_key = userdata.get(\"key_ptn\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN8RUtBIJXjJ"
      },
      "source": [
        "### Define file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuBXzDslJXjK"
      },
      "outputs": [],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzoOTBUcJXjK"
      },
      "outputs": [],
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVdViu7WJXjK"
      },
      "source": [
        "### Read PDF to string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VslYbViqJXjL"
      },
      "outputs": [],
      "source": [
        "content = read_pdf_to_string(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqheLXGFJXjL"
      },
      "source": [
        "### Breakpoint types:\n",
        "* 'percentile': all differences between sentences are calculated, and then any difference greater than the X percentile is split.\n",
        "* 'standard_deviation': any difference greater than X standard deviations is split.\n",
        "* 'interquartile': the interquartile distance is used to split chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrfNm0kCJXjL"
      },
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(OpenAIEmbeddings(), breakpoint_threshold_type='percentile', breakpoint_threshold_amount=90) # chose which embeddings and breakpoint type and threshold to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Yh_Z3B6JXjL"
      },
      "source": [
        "### Split original text to semantic chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxHnZCRVJXjL"
      },
      "outputs": [],
      "source": [
        "docs = text_splitter.create_documents([content])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR8OUtWTJXjL"
      },
      "source": [
        "### Create vector store and retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsZpqHeiJXjM"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "chunks_query_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MJcLKAFJXjM"
      },
      "source": [
        "### Test the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD7zw66IJXjM"
      },
      "outputs": [],
      "source": [
        "test_query = \"What is the main cause of climate change?\"\n",
        "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n",
        "show_context(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nKQeeLDJXjM"
      },
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--semantic-chunking)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}